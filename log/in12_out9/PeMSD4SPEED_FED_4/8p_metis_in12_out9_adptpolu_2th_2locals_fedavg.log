2025-04-07 12:47: Experiment log path in: /data/zm/2025code/log/in12_out9
2025-04-07 12:47: Train: (10183, 12, 37, 3), (10183, 9, 37, 3)
2025-04-07 12:47: Val: (3394, 12, 37, 3), (3394, 9, 37, 3)
2025-04-07 12:47: Test: (3395, 12, 37, 3), (3395, 9, 37, 3)
2025-04-07 12:47: memory_usage: (0.837890625, 2.0)
2025-04-07 12:47: Applying learning rate decay.
2025-04-07 12:47: *****************Model Parameter*****************
2025-04-07 12:47: data_l torch.Size([96]) True
2025-04-07 12:47: tod_embedding.weight torch.Size([288, 32]) True
2025-04-07 12:47: dow_embedding.weight torch.Size([7, 32]) True
2025-04-07 12:47: SpatialEmbedding.B_sp torch.Size([307, 32]) True
2025-04-07 12:47: SpatialEmbedding.B_su torch.Size([307, 32]) True
2025-04-07 12:47: data_embedding.weight torch.Size([96, 1]) True
2025-04-07 12:47: data_embedding.bias torch.Size([96]) True
2025-04-07 12:47: MLPA.blocks.0.linear.weight torch.Size([64, 64]) True
2025-04-07 12:47: MLPA.blocks.0.linear.bias torch.Size([64]) True
2025-04-07 12:47: MLPA.blocks.0.norm.weight torch.Size([64]) True
2025-04-07 12:47: MLPA.blocks.0.norm.bias torch.Size([64]) True
2025-04-07 12:47: MLPB.blocks.0.linear.weight torch.Size([64, 64]) True
2025-04-07 12:47: MLPB.blocks.0.linear.bias torch.Size([64]) True
2025-04-07 12:47: MLPB.blocks.0.norm.weight torch.Size([64]) True
2025-04-07 12:47: MLPB.blocks.0.norm.bias torch.Size([64]) True
2025-04-07 12:47: MLPC.blocks.0.linear.weight torch.Size([96, 96]) True
2025-04-07 12:47: MLPC.blocks.0.linear.bias torch.Size([96]) True
2025-04-07 12:47: MLPC.blocks.0.norm.weight torch.Size([96]) True
2025-04-07 12:47: MLPC.blocks.0.norm.bias torch.Size([96]) True
2025-04-07 12:47: MLPD.blocks.0.linear.weight torch.Size([128, 128]) True
2025-04-07 12:47: MLPD.blocks.0.linear.bias torch.Size([128]) True
2025-04-07 12:47: MLPD.blocks.0.norm.weight torch.Size([128]) True
2025-04-07 12:47: MLPD.blocks.0.norm.bias torch.Size([128]) True
2025-04-07 12:47: MLPE.blocks.0.linear.weight torch.Size([224, 224]) True
2025-04-07 12:47: MLPE.blocks.0.linear.bias torch.Size([224]) True
2025-04-07 12:47: MLPE.blocks.0.norm.weight torch.Size([224]) True
2025-04-07 12:47: MLPE.blocks.0.norm.bias torch.Size([224]) True
2025-04-07 12:47: MLPE.blocks.1.linear.weight torch.Size([224, 224]) True
2025-04-07 12:47: MLPE.blocks.1.linear.bias torch.Size([224]) True
2025-04-07 12:47: MLPE.blocks.1.norm.weight torch.Size([224]) True
2025-04-07 12:47: MLPE.blocks.1.norm.bias torch.Size([224]) True
2025-04-07 12:47: MLPE.blocks.2.linear.weight torch.Size([224, 224]) True
2025-04-07 12:47: MLPE.blocks.2.linear.bias torch.Size([224]) True
2025-04-07 12:47: MLPE.blocks.2.norm.weight torch.Size([224]) True
2025-04-07 12:47: MLPE.blocks.2.norm.bias torch.Size([224]) True
2025-04-07 12:47: MLPF.blocks.0.linear.weight torch.Size([12, 12]) True
2025-04-07 12:47: MLPF.blocks.0.linear.bias torch.Size([12]) True
2025-04-07 12:47: MLPF.blocks.0.norm.weight torch.Size([12]) True
2025-04-07 12:47: MLPF.blocks.0.norm.bias torch.Size([12]) True
2025-04-07 12:47: MLPF.blocks.1.linear.weight torch.Size([12, 12]) True
2025-04-07 12:47: MLPF.blocks.1.linear.bias torch.Size([12]) True
2025-04-07 12:47: MLPF.blocks.1.norm.weight torch.Size([12]) True
2025-04-07 12:47: MLPF.blocks.1.norm.bias torch.Size([12]) True
2025-04-07 12:47: steps_linear.weight torch.Size([9, 12]) True
2025-04-07 12:47: steps_linear.bias torch.Size([9]) True
2025-04-07 12:47: out_linear.weight torch.Size([1, 224]) True
2025-04-07 12:47: out_linear.bias torch.Size([1]) True
2025-04-07 12:47: Total params num: 217470
2025-04-07 12:47: *****************Finish Parameter****************
2025-04-07 12:47: Train Epoch 1: 0/159 Loss: 74.007729
2025-04-07 12:47: Train Epoch 1: 20/159 Loss: 15.543596
2025-04-07 12:47: Train Epoch 1: 40/159 Loss: 10.157866
2025-04-07 12:47: Train Epoch 1: 60/159 Loss: 10.266322
2025-04-07 12:48: Train Epoch 1: 80/159 Loss: 8.679010
2025-04-07 12:48: Train Epoch 1: 100/159 Loss: 8.649017
2025-04-07 12:48: Train Epoch 1: 120/159 Loss: 6.679350
2025-04-07 12:48: Train Epoch 1: 140/159 Loss: 7.193250
2025-04-07 12:48: **********Train Epoch 1: Average Loss: 11.149243
2025-04-07 12:48: **********Train Epoch 1: MAE: 11.149243 RMSE: 15.399410 MAPE: 0.177658
2025-04-07 12:48: **********Val Epoch 1: Average Loss: 4.718916
2025-04-07 12:48: **********Val Epoch 1: MAE: 4.718916 RMSE: 8.826335 MAPE: 0.087611
2025-04-07 12:48: *********************************Current best model saved!
2025-04-07 12:48: Train Epoch 1: 0/159 Loss: 6.943489
2025-04-07 12:48: Train Epoch 1: 20/159 Loss: 6.693388
2025-04-07 12:48: Train Epoch 1: 40/159 Loss: 5.890592
2025-04-07 12:48: Train Epoch 1: 60/159 Loss: 6.205911
2025-04-07 12:48: Train Epoch 1: 80/159 Loss: 5.121888
2025-04-07 12:48: Train Epoch 1: 100/159 Loss: 5.493605
2025-04-07 12:48: Train Epoch 1: 120/159 Loss: 6.064946
2025-04-07 12:48: Train Epoch 1: 140/159 Loss: 5.060840
2025-04-07 12:48: **********Train Epoch 1: Average Loss: 5.853899
2025-04-07 12:48: **********Train Epoch 1: MAE: 5.853899 RMSE: 9.242893 MAPE: 0.103970
2025-04-07 12:49: **********Val Epoch 1: Average Loss: 3.706554
2025-04-07 12:49: **********Val Epoch 1: MAE: 3.706554 RMSE: 7.189650 MAPE: 0.071812
2025-04-07 12:49: *********************************Current best model saved!
2025-04-07 12:49: Train Epoch 2: 0/159 Loss: 5.689246
2025-04-07 12:49: Train Epoch 2: 20/159 Loss: 4.853343
2025-04-07 12:49: Train Epoch 2: 40/159 Loss: 4.253479
2025-04-07 12:49: Train Epoch 2: 60/159 Loss: 4.143673
2025-04-07 12:49: Train Epoch 2: 80/159 Loss: 3.819347
2025-04-07 12:49: Train Epoch 2: 100/159 Loss: 3.771526
2025-04-07 12:49: Train Epoch 2: 120/159 Loss: 3.564832
2025-04-07 12:49: Train Epoch 2: 140/159 Loss: 3.390160
2025-04-07 12:49: **********Train Epoch 2: Average Loss: 4.187158
2025-04-07 12:49: **********Train Epoch 2: MAE: 4.187158 RMSE: 6.841782 MAPE: 0.077895
2025-04-07 12:49: **********Val Epoch 2: Average Loss: 2.524863
2025-04-07 12:49: **********Val Epoch 2: MAE: 2.524863 RMSE: 5.129960 MAPE: 0.050571
2025-04-07 12:49: *********************************Current best model saved!
2025-04-07 12:49: Train Epoch 2: 0/159 Loss: 3.217513
2025-04-07 12:49: Train Epoch 2: 20/159 Loss: 3.212218
2025-04-07 12:49: Train Epoch 2: 40/159 Loss: 3.140599
2025-04-07 12:50: Train Epoch 2: 60/159 Loss: 3.007281
2025-04-07 12:50: Train Epoch 2: 80/159 Loss: 2.843674
2025-04-07 12:50: Train Epoch 2: 100/159 Loss: 2.655337
2025-04-07 12:50: Train Epoch 2: 120/159 Loss: 2.124369
2025-04-07 12:50: Train Epoch 2: 140/159 Loss: 2.874296
2025-04-07 12:50: **********Train Epoch 2: Average Loss: 2.830688
2025-04-07 12:50: **********Train Epoch 2: MAE: 2.830688 RMSE: 5.162604 MAPE: 0.054688
2025-04-07 12:50: **********Val Epoch 2: Average Loss: 2.183399
2025-04-07 12:50: **********Val Epoch 2: MAE: 2.183399 RMSE: 4.646938 MAPE: 0.043593
2025-04-07 12:50: *********************************Current best model saved!
2025-04-07 12:50: Train Epoch 3: 0/159 Loss: 2.988755
2025-04-07 12:50: Train Epoch 3: 20/159 Loss: 3.471450
2025-04-07 12:50: Train Epoch 3: 40/159 Loss: 2.669496
2025-04-07 12:50: Train Epoch 3: 60/159 Loss: 2.444592
2025-04-07 12:50: Train Epoch 3: 80/159 Loss: 2.558690
2025-04-07 12:50: Train Epoch 3: 100/159 Loss: 2.320848
2025-04-07 12:50: Train Epoch 3: 120/159 Loss: 2.141021
2025-04-07 12:51: Train Epoch 3: 140/159 Loss: 2.240609
2025-04-07 12:51: **********Train Epoch 3: Average Loss: 2.781181
2025-04-07 12:51: **********Train Epoch 3: MAE: 2.781181 RMSE: 5.055887 MAPE: 0.053419
2025-04-07 12:51: **********Val Epoch 3: Average Loss: 2.129976
2025-04-07 12:51: **********Val Epoch 3: MAE: 2.129976 RMSE: 4.587290 MAPE: 0.042537
2025-04-07 12:51: *********************************Current best model saved!
2025-04-07 12:51: Train Epoch 3: 0/159 Loss: 2.231595
2025-04-07 12:51: Train Epoch 3: 20/159 Loss: 2.646907
2025-04-07 12:51: Train Epoch 3: 40/159 Loss: 2.551473
2025-04-07 12:51: Train Epoch 3: 60/159 Loss: 2.057404
2025-04-07 12:51: Train Epoch 3: 80/159 Loss: 2.052167
2025-04-07 12:51: Train Epoch 3: 100/159 Loss: 2.417690
2025-04-07 12:51: Train Epoch 3: 120/159 Loss: 2.148698
2025-04-07 12:51: Train Epoch 3: 140/159 Loss: 2.227880
